{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QES802c1c78S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datasets import Dataset\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5MFGJJneDd6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from notebooks.EDA import cleaning_and_processing_testdata, cleaning_and_processing, clean_tweet"
      ],
      "metadata": {
        "id": "o3wzENjCyFQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAfiLMgNsXEX"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#Please : i renamed the dfs to add clarity\n",
        "obama_df = pd.read_excel('/content/training-Obama-Romney-tweets.xlsx', sheet_name='Obama')\n",
        "romney_df = pd.read_excel('/content/training-Obama-Romney-tweets.xlsx', sheet_name='Romney')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUdMinErl0Jk"
      },
      "outputs": [],
      "source": [
        "def train_roberta_for_candidate(candidate: str):\n",
        "    \"\"\"\n",
        "    Please : call this with 'Obama' or 'Romney'.\n",
        "    It will:\n",
        "      - use the corresponding df (obama_df or romney_df)\n",
        "      - prepare the dataset\n",
        "      - fine-tune Cardiff twitter RoBERTa\n",
        "      - print eval metrics + classification report (-1,0,1)\n",
        "      - save the model + tokenizer in ./models/roberta_<candidate>_cardiff\n",
        "    \"\"\"\n",
        "    name = candidate.strip().capitalize()\n",
        "    if name == \"Obama\":\n",
        "        base_df = obama_df\n",
        "    elif name == \"Romney\":\n",
        "        base_df = romney_df\n",
        "    else:\n",
        "        raise ValueError(\"candidate must be 'Obama' or 'Romney'\")\n",
        "\n",
        "    print(f\"\\n===== Training RoBERTa (Cardiff) for {name} tweets =====\\n\")\n",
        "\n",
        "    #Helps us map our dataset labels to/from roberta labels\n",
        "    label_to_id = {-1: 0, 0: 1, 1: 2}\n",
        "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
        "\n",
        "    #keep only labels -1,0,1 and add numeric label_id\n",
        "    exp_df = base_df[base_df[\"label\"].isin([-1, 0, 1])].copy()\n",
        "    exp_df[\"label_id\"] = exp_df[\"label\"].map(label_to_id).astype(int)\n",
        "\n",
        "    #apply cleaning and get labels list to use in the next cells\n",
        "    texts = exp_df[\"clean_tweet\"].astype(str).tolist()\n",
        "    labels = exp_df[\"label_id\"].tolist()\n",
        "\n",
        "    #Training and testing split\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        texts,\n",
        "        labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=labels,\n",
        "    )\n",
        "    '''\n",
        "    # trying resampling ------------------------------------------------------------\n",
        "    # df must contain a column \"label\"\n",
        "    df = X_train.copy()\n",
        "\n",
        "    neg = df[df.label_id == 0]\n",
        "    neu = df[df.label_id == 1]\n",
        "    pos = df[df.label_id == 2]\n",
        "\n",
        "    # Find the largest class size\n",
        "    max_size = max(len(neg), len(neu), len(pos))\n",
        "\n",
        "    neg_over = resample(neg, replace=True, n_samples=max_size, random_state=42)\n",
        "    neu_over = resample(neu, replace=True, n_samples=max_size, random_state=42)\n",
        "    pos_over = resample(pos, replace=True, n_samples=max_size, random_state=42)\n",
        "\n",
        "    X_train = pd.concat([neg_over, neu_over, pos_over]).sample(frac=1, random_state=42)\n",
        "\n",
        "    ---------------------------------------------------------------------------------\n",
        "    '''\n",
        "\n",
        "    train_ds = Dataset.from_dict({\"text\": X_train, \"label\": y_train})\n",
        "    val_ds   = Dataset.from_dict({\"text\": X_val,   \"label\": y_val})\n",
        "\n",
        "    #load RoBERTa tokenizer\n",
        "    model_path = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    def tokenize_batch(batch):\n",
        "        return tokenizer(\n",
        "            batch[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=96,  # Please : adjust later if needed\n",
        "        )\n",
        "\n",
        "    #apply to train and validation sets\n",
        "    train_ds_tokenized = train_ds.map(tokenize_batch, batched=True)\n",
        "    val_ds_tokenized   = val_ds.map(tokenize_batch, batched=True)\n",
        "\n",
        "    #format for PyTorch\n",
        "    train_ds_tokenized = train_ds_tokenized.remove_columns([\"text\"])\n",
        "    val_ds_tokenized   = val_ds_tokenized.remove_columns([\"text\"])\n",
        "\n",
        "    train_ds_tokenized.set_format(\"torch\")\n",
        "    val_ds_tokenized.set_format(\"torch\")\n",
        "\n",
        "    #Metrics function for Trainer\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels_np = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "        acc = accuracy_score(labels_np, preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            labels_np, preds, average=\"macro\", zero_division=0\n",
        "        )\n",
        "        return {\n",
        "            \"accuracy\": acc,\n",
        "            \"macro_f1\": f1,\n",
        "            \"macro_precision\": precision,\n",
        "            \"macro_recall\": recall,\n",
        "        }\n",
        "\n",
        "    #load RoBERTa model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_path,\n",
        "        num_labels=3,  # classes: 0,1,2 corresponding to -1,0,1\n",
        "    )\n",
        "\n",
        "    #Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./roberta_{name.lower()}_cardiff\",   #Please : folder where checkpoints/logs are saved\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        num_train_epochs=4,                    #will change later in tuning\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=32,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=50,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    #Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds_tokenized,\n",
        "        eval_dataset=val_ds_tokenized,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
        "    )\n",
        "\n",
        "    #Train and evaluate\n",
        "    #Please : this tunes the Cardiff twitter roberta model on {name} tweets\n",
        "    trainer.train()\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"==== RoBERTa (Cardiff) {name} eval metrics ====\")\n",
        "    print(eval_results)\n",
        "\n",
        "    #Detailed classification report in terms of -1,0,1\n",
        "    pred_output = trainer.predict(val_ds_tokenized)\n",
        "    logits = pred_output.predictions\n",
        "    pred_ids = np.argmax(logits, axis=-1)\n",
        "\n",
        "    true_ids = np.array(y_val)  # y_val was label_id from earlier split (0,1,2)\n",
        "\n",
        "    # map back 0,1,2 to -1,0,1 using id_to_label defined earlier\n",
        "    true_labels = [id_to_label[i] for i in true_ids]\n",
        "    pred_labels = [id_to_label[i] for i in pred_ids]\n",
        "\n",
        "    print(f\"\\n==== RoBERTa (Cardiff) {name} - classification report (labels -1,0,1) ====\\n\")\n",
        "    print(classification_report(true_labels, pred_labels))\n",
        "\n",
        "    #Save the fine-tuned model for later\n",
        "    save_dir = f\"./models/roberta_{name.lower()}_cardiff\"\n",
        "    trainer.save_model(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(f\"\\nSaved fine-tuned {name} RoBERTa model to: {save_dir}\")\n",
        "\n",
        "    return trainer, eval_results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for Obama\n",
        "trainer_obama, obama_results = train_roberta_for_candidate(\"Obama\")"
      ],
      "metadata": {
        "id": "Nj21ICSu1GYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for Romney\n",
        "trainer_romney, romney_results = train_roberta_for_candidate(\"Romney\")"
      ],
      "metadata": {
        "id": "fbw5b--61IhZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}