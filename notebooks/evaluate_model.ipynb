{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QES802c1c78S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from datasets import Dataset\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5MFGJJneDd6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        ")\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IOMPOhA2Dl2"
      },
      "outputs": [],
      "source": [
        "from notebooks.train_model import train_roberta_for_candidate\n",
        "from notebooks.EDA import cleaning_and_processing_testdata, cleaning_and_processing, clean_tweet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for Obama\n",
        "trainer_obama, obama_results = train_roberta_for_candidate(\"Obama\")"
      ],
      "metadata": {
        "id": "oh3yFbj62MKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for Romney\n",
        "trainer_romney, romney_results = train_roberta_for_candidate(\"Romney\")"
      ],
      "metadata": {
        "id": "SRfch6Q32R5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW5u9tah3qmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0816cc85-1e30-4ac1-ea91-91ef0fc3550a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Running prediction pipeline for Obama =====\n",
            "Saved predictions for Obama to obama_predictions.txt\n",
            "\n",
            "===== Running prediction pipeline for Romney =====\n",
            "Saved predictions for Romney to romney_predictions.txt\n"
          ]
        }
      ],
      "source": [
        "# id_to_label is already defined globally as {0: -1, 1: 0, 2: 1}\n",
        "# clean_tweet and cleaning_and_processing functions are also globally defined.\n",
        "\n",
        "def prepare_candidate_test_df(excel_path, candidate_name):\n",
        "    \"\"\"\n",
        "    Load the specified candidate's sheet from the given Excel file,\n",
        "    apply the same cleaning as for training, and return a dataframe\n",
        "    with a clean_tweet column ready for prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    # FINAL TEST DATA HAS NO HEADER → we provide names manually\n",
        "    df = pd.read_excel(excel_path, sheet_name=candidate_name, header=None, names=[\"tweet_number\", \"tweet\"])\n",
        "\n",
        "    # remove any HTML-like tags BEFORE clean_tweet\n",
        "    df[\"tweet\"] = df[\"tweet\"].astype(str).str.replace(r'<[^>]+>', '', regex=True)\n",
        "\n",
        "    # drop rows with no tweet text (safety)\n",
        "    df = df.dropna(subset=[\"tweet\"]).reset_index(drop=True)\n",
        "\n",
        "    # use SAME cleaning function as training\n",
        "    df[\"clean_tweet\"] = df[\"tweet\"].apply(clean_tweet)\n",
        "\n",
        "    return df\n",
        "\n",
        "def load_candidate_roberta_model(candidate_name):\n",
        "    \"\"\"\n",
        "    Load the fine-tuned RoBERTa model for the specified candidate.\n",
        "    \"\"\"\n",
        "    name_lower = candidate_name.lower()\n",
        "    model_dir = f\"./models/roberta_{name_lower}_cardiff\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "    return tokenizer, model\n",
        "\n",
        "def predict_labels_for_candidate(df, tokenizer, model, batch_size=32):\n",
        "    \"\"\"\n",
        "    Given a cleaned dataframe with 'clean_tweet',\n",
        "    run RoBERTa in batches and add a 'pred_label' column with values -1,0,1.\n",
        "    \"\"\"\n",
        "    texts = df[\"clean_tweet\"].astype(str).tolist()\n",
        "    all_pred_ids = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        enc = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=96,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**enc)\n",
        "            logits = outputs.logits\n",
        "            batch_ids = torch.argmax(logits, dim=-1).cpu().numpy().tolist()\n",
        "            all_pred_ids.extend(batch_ids)\n",
        "\n",
        "    # map 0,1,2 → -1,0,1\n",
        "    df[\"pred_label\"] = [id_to_label[i] for i in all_pred_ids]\n",
        "    return df[['tweet', 'pred_label']]\n",
        "\n",
        "def run_candidate_pipeline_on_sample_for_output(candidate_name, sample_path):\n",
        "    print(f\"\\n===== Running prediction pipeline for {candidate_name} =====\")\n",
        "\n",
        "    # Prepare test data\n",
        "    df = prepare_candidate_test_df(sample_path, candidate_name)\n",
        "\n",
        "    '''\n",
        "    # Handle overlapping tweets\n",
        "    training_df = obama_df if candidate_name == \"Obama\" else romney_df\n",
        "    training_tweets_set = set(training_df[\"clean_tweet\"].astype(str).tolist())\n",
        "    initial_sample_count = len(df)\n",
        "    df_filtered = df[~df[\"clean_tweet\"].astype(str).isin(training_tweets_set)].reset_index(drop=True)\n",
        "    removed_count = initial_sample_count - len(df_filtered)\n",
        "    print(f\"Removed {removed_count} tweets from sample-testdata ({candidate_name} sheet) that were present in {candidate_name}_df training data.\")\n",
        "\n",
        "    if df_filtered.empty:\n",
        "        print(f\"No unique tweets remaining for {candidate_name} after removing overlaps. Skipping prediction.\")\n",
        "        return\n",
        "    '''\n",
        "\n",
        "    # Load model\n",
        "    tokenizer, model = load_candidate_roberta_model(candidate_name)\n",
        "\n",
        "    # Make predictions\n",
        "    df_pred = predict_labels_for_candidate(df.copy(), tokenizer, model)\n",
        "\n",
        "    # Format and save to .txt file\n",
        "    output_filename = f\"{candidate_name.lower()}_predictions.txt\"\n",
        "    with open(output_filename, \"w\") as f:\n",
        "        f.write(\"(setf x *(\\n\")\n",
        "        # Line numbers start from 1\n",
        "        for idx, row in df_pred.reset_index(drop=True).iterrows():\n",
        "            f.write(f\"({idx + 1} {row['pred_label']})\\n\")\n",
        "        f.write(\") )\\n\")\n",
        "    print(f\"Saved predictions for {candidate_name} to {output_filename}\")\n",
        "\n",
        "# Main execution calls\n",
        "# The sample_path is already defined as '/content/sample-testdata.xlsx'\n",
        "\n",
        "# Run for Obama\n",
        "obama_predictions = run_candidate_pipeline_on_sample_for_output(\"Obama\", obama_path)\n",
        "\n",
        "# Run for Romney\n",
        "romney_predictions = run_candidate_pipeline_on_sample_for_output(\"Romney\", romney_path)\n"
      ]
    }
  ]
}